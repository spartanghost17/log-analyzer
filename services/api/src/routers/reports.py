"""
Reports Router
Endpoints for LLM-generated analysis reports
"""

from typing import Optional, List

from fastapi import APIRouter, Depends, Query, HTTPException
from pydantic import BaseModel

from ..services.database import DatabaseService
from ..services.cache import CacheService

router = APIRouter()


# ============================================================================
# Pydantic Models
# ============================================================================

class AnalysisReport(BaseModel):
    """LLM-generated analysis report"""
    id: int
    analysis_type: str
    time_window_start: str
    time_window_end: str
    services_analyzed: List[str]
    total_errors: int
    unique_patterns: int
    severity: str
    summary: str
    key_findings: List[str]
    recommendations: List[str]
    created_at: str


class ReportsResponse(BaseModel):
    """Response for reports query"""
    reports: List[AnalysisReport]
    total: int
    limit: int
    offset: int


# ============================================================================
# Dependency Injection
# ============================================================================

def get_db() -> DatabaseService:
    """Get database service from app state"""
    from ..app import db_service
    return db_service


def get_cache() -> CacheService:
    """Get cache service from app state"""
    from ..app import cache_service
    return cache_service


# ============================================================================
# Endpoints
# ============================================================================

@router.get("/", response_model=ReportsResponse)
async def get_reports(
        limit: int = Query(10, ge=1, le=100),
        offset: int = Query(0, ge=0),
        db: DatabaseService = Depends(get_db),
        cache: CacheService = Depends(get_cache)
):
    """
    Get LLM-generated analysis reports

    - **limit**: Number of reports to return (1-100)
    - **offset**: Offset for pagination

    Returns analysis reports generated by the LLM analyzer service.
    """

    # Try cache
    cache_key = cache.cache_key("reports", limit=limit, offset=offset)
    cached = await cache.get(cache_key)
    if cached:
        return cached

    # Query database
    reports = await db.get_analysis_reports(limit=limit, offset=offset)
    total = await db.get_report_count()

    response = ReportsResponse(
        reports=[AnalysisReport(**r) for r in reports],
        total=total,
        limit=limit,
        offset=offset
    )

    # Cache for 5 minutes
    await cache.set(cache_key, response.model_dump(), ttl=300)

    return response


@router.get("/latest", response_model=Optional[AnalysisReport])
async def get_latest_report(
        db: DatabaseService = Depends(get_db),
        cache: CacheService = Depends(get_cache)
):
    """Get the most recent analysis report"""

    # Try cache
    cache_key = "reports:latest"
    cached = await cache.get(cache_key)
    if cached:
        return cached

    # Query database
    report = await db.get_latest_report()

    if not report:
        return None

    response = AnalysisReport(**report)

    # Cache for 5 minutes
    await cache.set(cache_key, response.model_dump(), ttl=300)

    return response


@router.get("/{report_id}", response_model=AnalysisReport)
async def get_report_by_id(
        report_id: int,
        db: DatabaseService = Depends(get_db),
        cache: CacheService = Depends(get_cache)
):
    """Get a specific analysis report by ID"""

    # Try cache
    cache_key = f"reports:{report_id}"
    cached = await cache.get(cache_key)
    if cached:
        return cached

    # Query database by ID
    report = await db.get_report_by_id(report_id)

    if not report:
        raise HTTPException(status_code=404, detail="Report not found")

    response = AnalysisReport(**report)

    # Cache for 30 minutes
    await cache.set(cache_key, response.model_dump(), ttl=1800)

    return response